YOLO mode is enabled. All tool calls will be automatically approved.
YOLO mode is enabled. All tool calls will be automatically approved.
I will write the ETL pipeline script into a Jupyter Notebook file at `/home/user/etl_pipeline.ipynb`. This notebook will use `pandas` and `sqlite3` to perform the requested data transformation and save it to the specified output CSV.

I'll execute the notebook to ensure it cleanly processes the data without any runtime errors.
I will inspect the file that was written to identify the syntax error.
I will correct the JSON structure of the notebook by writing it using a short Python script to handle the escaping properly.
I will now run the notebook again to verify that it executes properly and generates the correct output CSV file.
I will check the content of the `clean_sales.csv` to confirm the required data and columns were properly written.
The ETL pipeline has been successfully created and verified.

The Jupyter Notebook at `/home/user/etl_pipeline.ipynb` was generated with the correct schema, executing the following logical steps:
1. Imports `pandas` and `sqlite3`.
2. Reads the raw sales CSV data into a pandas DataFrame.
3. Sets up an in-memory SQLite database (`sqlite3.connect(':memory:')`) and writes the dataframe to a `sales` table.
4. Executes a SQL query leveraging the `RANK() OVER(ORDER BY SUM(amount) DESC)` window function to group by `region`, calculate `total_revenue`, and determine the `sales_rank`.
5. Outputs the final dataframe directly to `/home/user/clean_sales.csv` without an index.

I have executed it locally via `nbconvert` and verified that the `clean_sales.csv` output perfectly matches your requested schema containing exactly `region`, `total_revenue`, and `sales_rank`.
