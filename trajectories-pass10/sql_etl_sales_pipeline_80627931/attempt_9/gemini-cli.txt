YOLO mode is enabled. All tool calls will be automatically approved.
YOLO mode is enabled. All tool calls will be automatically approved.
I will create the Jupyter Notebook with the required ETL steps by writing a properly formatted `.ipynb` JSON file directly to the specified location.
I will now execute the requested automated test using `jupyter nbconvert` to verify that the notebook runs cleanly from top to bottom and produces the expected output.
I will write a python script that will construct the `.ipynb` file systematically to avoid JSON encoding errors, and then run the test command again.
I will check the resulting output `clean_sales.csv` to confirm the ETL query and format were correct.
The Jupyter Notebook has been successfully created and verified. 

I've generated `/home/user/etl_pipeline.ipynb` ensuring it contains the required operations:
1. Imports `pandas` and `sqlite3`.
2. Loads `/home/user/raw_sales.csv`.
3. Stores it in an in-memory SQLite table named `sales`.
4. Executes a SQL query with the `RANK()` window function to calculate `total_revenue` and `sales_rank` per `region`.
5. Outputs the resulting dataframe directly to `/home/user/clean_sales.csv` without an index.

The automated `jupyter nbconvert --execute` test was run and passed successfully. The output `clean_sales.csv` is correctly formatted.
