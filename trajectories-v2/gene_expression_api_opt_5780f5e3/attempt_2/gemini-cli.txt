YOLO mode is enabled. All tool calls will be automatically approved.
YOLO mode is enabled. All tool calls will be automatically approved.
I am going to read the existing Jupyter notebook to see its current implementation and how it fetches the data.
I will also check the `mock_api.py` module to understand what the `get_expression_data` function returns.
I will rewrite the Jupyter notebook to use `ThreadPoolExecutor` for concurrent data fetching, which will drastically reduce the execution time.
Now I will execute the updated Jupyter notebook to run the concurrent fetching process and generate the requested `expression_data.csv` output.
I will check the resulting output `expression_data.csv` file to ensure the script ran correctly, fetched the correct number of rows, and formatted the data as requested.
I will read the generated `.nbconvert.ipynb` to check the execution time.
I have optimized `/home/user/fetch_data.ipynb` by using `ThreadPoolExecutor` from `concurrent.futures` with 20 workers to fetch the 20 pages in parallel. The resulting data is then assembled into a Pandas DataFrame, sorted by `gene_id` in ascending order, formatted to guarantee the correct column order (`gene_id`, `expression_level`, `sample_id`), and saved to `/home/user/expression_data.csv`. 

I also executed the notebook via `jupyter nbconvert`, which successfully reduced the execution time from ~10 seconds down to ~0.5 seconds, satisfying your requirements. The 200 data rows have been generated correctly.
